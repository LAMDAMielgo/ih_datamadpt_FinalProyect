{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA FROM CATASTRO INSPIRE\n",
    "___________________________________________________________________________________________________\n",
    "\n",
    "Obteined through qGIS after mingling from a while with:\n",
    "- WMS service: http://ovc.catastro.meh.es/cartografia/INSPIRE/spadgcwms.aspx\n",
    "- WFS buildings : http://ovc.catastro.meh.es/INSPIRE/wfsBU.aspx?\n",
    "- todos los serivicios INSPIRE: http://www.catastro.minhap.es/webinspire/index.html\n",
    "\n",
    "*Notas: el uso de los canales WMS/WFS devuelven las capas antes de unificar con el programa Europeo Inspire, por lo que quizá sea la razón por la que no funcionan bien.\n",
    "\n",
    "Existen varios GitHubs enfocados a consultas del catastro con python:\n",
    "- **[pyCatastro](#https://github.com/gisce/pycatastro)**: permite realizar consultas en formato API. \n",
    ">>- No permite la descarga total de datos en función de municipio y provincia\n",
    ">>- Devuelve diccionarios\n",
    ">>- Para descargarme Madrid, debería obtener (1) Todas las vías, (2) Todas las siglas e iterar, haciendo mogollón de llamadas (not a good idea)\n",
    "\n",
    "- **[catastro-lib-python](#https://github.com/sperea/catastro-lib-python)**: parece un antecersor del anterior. No se ha probado pues se dejó de actualizar en 2018\n",
    "- **[Catastro Inspire Downloader](#https://github.com/geomatico/cidownloader)**: realizados por los mismos cartógrafos y desarrolladores que el complemento de qGIS, permite descargar datos en formato .geopackage\n",
    ">>- Permite descargar según provincia, municipio y proyección\n",
    ">>- Es poco consistente: al descargar Madrid a veces devuelve sólo datos de CadastralParcel o de BuildingParts. Comparando con los gmls incluidos en ZIPs que se pueden descargar a parte, los datos son incompletos. Posiblemente se un problema con la librería GDAL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import combinations\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import geopandas as gpd\n",
    "import geojson\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONAL PIPELINE PROTOTYPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------ checking which columns should be purged\n",
    "\n",
    "def str_forUniques(num):\n",
    "    \"\"\"\n",
    "    Return different string depending of unique_len in checking_forUniques\n",
    "    \"\"\"\n",
    "    if num == 0: return 'ALL NULLS'\n",
    "    else: return 'Unique items'\n",
    "\n",
    "def checking_forUniques(gdf):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    output:\n",
    "    \"\"\"\n",
    "    cols_with_one_element = []\n",
    "    \n",
    "    print(f\"\\n-------------------- Current Layers in {gdf.name} ------------------------\")\n",
    "    print(f\"------------------------------------------------------------------------\")\n",
    "    \n",
    "    for i,col in enumerate(gdf.columns.tolist()):\n",
    "        if (col != 'geometry'):\n",
    "            unique_len = len(gdf[str(col)].value_counts().tolist())\n",
    "            \n",
    "            if unique_len == 0: \n",
    "                print(f\"{i+1}. {col}:\\t\\t\\t{unique_len}\\t{str_forUniques(unique_len)}\")                \n",
    "            elif len(col) <= 12 and unique_len != 0: \n",
    "                print(f\"{i+1}. {col}:\\t\\t\\t\\t\\t{unique_len}\\t{str_forUniques(unique_len)}\")\n",
    "            elif 12 < len(col) <= 19 and unique_len != 0: \n",
    "                print(f\"{i+1}. {col}:\\t\\t\\t\\t{unique_len}\\t{str_forUniques(unique_len)}\")\n",
    "            elif 19 < len(col) <= 28 and unique_len != 0: \n",
    "                print(f\"{i+1}. {col}:\\t\\t\\t{unique_len}\\t{str_forUniques(unique_len)}\")\n",
    "            elif 28 < len(col) <= 36 and unique_len != 0: \n",
    "                print(f\"{i+1}. {col}:\\t\\t{unique_len}\\t{str_forUniques(unique_len)}\")\n",
    "            elif 36 < len(col) and unique_len != 0: \n",
    "                print(f\"{i+1}. {col}:\\t{unique_len}\\t{str_forUniques(unique_len)}\")\n",
    "            else: pass\n",
    "            \n",
    "            if (unique_len == 1) or (unique_len == 0): cols_with_one_element.append(col)\n",
    "            else: pass            \n",
    "        else: pass\n",
    "    \n",
    "    print(f\"------------------------------------------------------------------------\\n\")\n",
    "    return cols_with_one_element\n",
    "\n",
    "def droping_DupCols(gdf, drop_cols = True):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"   \n",
    "    if drop_cols:\n",
    "        cols_to_drop = checking_forUniques(gdf)\n",
    "        \n",
    "        print(f\"-------------- Droping DUPLICATED COLUMNS in {gdf.name} ------------------\\n\")\n",
    "        [print(f'{i+1}. {col}\\v') for i, col in enumerate(cols_to_drop)] # repr without new line\n",
    "        \n",
    "        gdf.drop(cols_to_drop, \n",
    "                 axis=1, inplace = True)\n",
    "        \n",
    "        print(f\"-- Finished task -----------------------------------------------------\\n\")\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------ separate ID_parts if needed\n",
    "\n",
    "def get_part(x):\n",
    "    \"\"\"\n",
    "    input: col withs IDs_partXX\n",
    "    output: XX as int\n",
    "    Get numeric item in partXX from ID_partXX\n",
    "    \"\"\"\n",
    "    part_str = x.split('_')[1]\n",
    "    return int(part_str.split('t')[1])\n",
    "\n",
    "def get_ID(x):\n",
    "    \"\"\"\n",
    "    input: localID_partXX\n",
    "    output: localID\n",
    "    \"\"\"\n",
    "    return x.split('_')[0]\n",
    "\n",
    "def separate_parts(gdf, cols = ['']):\n",
    "    \"\"\"\n",
    "    If it is a geodf with ID_partXX then both parts are separated in different cols\n",
    "    This is necessary to be able to join gdfs\n",
    "    \"\"\"\n",
    "    print(f\"-------------- Checking for COLS to separate in {gdf.name} --------------\")\n",
    "    assert type(cols) == list\n",
    "    \n",
    "    c = 0\n",
    "    for col in cols:\n",
    "        if (len(re.findall(r\"_\", gdf[col].tolist()[0])) != 0):\n",
    "            \n",
    "            print(f\"{c+1}. {col}\\t\\t Dropped\")\n",
    "            \n",
    "            splited_col_name = re.split(r\"_\", gdf[col].tolist()[0])\n",
    "            part_title = re.findall(r\"\\D+\", splited_col_name[1])\n",
    "\n",
    "            gdf[col + f'_{part_title[0]}'] = gdf[col].apply(get_part).astype(dtype = 'int64')\n",
    "            gdf[col] = gdf[col].apply(get_ID)\n",
    "            c += 1\n",
    "        else: print(f\"No columns to separate\")\n",
    "            \n",
    "    print(f\"-- Finished task -----------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------ datetime operations\n",
    "\n",
    "def get_year(strng):\n",
    "    \"\"\"\n",
    "    Input:  string\n",
    "    Output: year as string\n",
    "    \n",
    "    Note_____________________________________________________________\n",
    "    Pandas requires years to be inside the bound of 1677 - 2262\n",
    "    To use pandas Timestamp it is need to defined custom Stamp Period\n",
    "    String operations seems easier in this case\n",
    "    \"\"\"\n",
    "    first_w = strng.split('T')[0]\n",
    "    return first_w.split('-')[0]\n",
    "\n",
    "def getYearOfConstruction(gdf, LifeSpanCol = 'beginLifespanVersion', drop_col = True):\n",
    "    \"\"\"\n",
    "    Cleaning Datetime\n",
    "    \"\"\"\n",
    "    print(f\"-- Getting YEAR OF CONSTRUCTION in {gdf.name} --------------------------\")   \n",
    "    gdf['yearOfConstruction'] = gdf[LifeSpanCol].apply(get_year)\n",
    "    \n",
    "    if drop_col and LifeSpanCol in gdf.columns.tolist():\n",
    "        print(f\"Droping col {drop_col}: \\t{LifeSpanCol}\")\n",
    "        gdf.drop(LifeSpanCol, axis = 1, inplace = True)        \n",
    "        \n",
    "    else: print(f\"Droping col {drop_col}: \\t{LifeSpanCol}\")\n",
    "    \n",
    "    print(f\"-- Finished task -----------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------ Droping duplicated columns\n",
    "\n",
    "def check_allTrue(gdf, col1, col2):\n",
    "    \"\"\"\n",
    "    Esta función se usa en ....\n",
    "    \"\"\"\n",
    "    print(f\"-- Checking if PAIRS are ALL TRUE {gdf.name} ---------------\")\n",
    "    \n",
    "    # hay columnas que son alturas y otras num de plantas. Con multiplicar x3 se arregla\n",
    "    if False not in gdf.apply(lambda x: (x[col1] == x[col2]) or (x[col1] == 3*x[col2]) or (3*x[col1] == x[col2]),\n",
    "                              axis = 1).value_counts().index.tolist():\n",
    "        \n",
    "        print(f\"All True --\\n-- Droping {col2}\")\n",
    "        gdf.drop([col2], axis = 1, inplace = True)\n",
    "    else:\n",
    "        print(f\"Pass \\tThere are inequalities between columns\")\n",
    "\n",
    "\n",
    "def checking_forIdenCols(gdf, drop_cols = True):\n",
    "    \"\"\"\n",
    "    Note_____________________________________________________________\n",
    "    Same unique elements are an indication that they give the same \n",
    "    (or nearly) the same information, therefore to simply ddbb\n",
    "    all columns that give the same info are purged\n",
    "    \"\"\"\n",
    "    print(f\"------------- Checking for SAME LEN COLS in {gdf.name} -----------------\")\n",
    "    \n",
    "    # 1 // creating vars for search\n",
    "    cols = [col for col in gdf.columns.tolist() if col != 'geometry']\n",
    "    len_unique_cols = [len(gdf[col].value_counts().tolist()) for col in cols]\n",
    "    equal_cols, del_cols = [], []\n",
    "    \n",
    "    # 2 // creating pairs of columns that are suspect of giving the same information\n",
    "    for tup_len, tup_col in zip(list(combinations(len_unique_cols, 2)), list(combinations(cols, 2))):\n",
    "        if tup_len[0] == tup_len[1]:\n",
    "            equal_cols.append([tup_col[0], tup_col[1]])\n",
    "        else: pass\n",
    "\n",
    "    # 3 // if True, drop columns that are equal, evaluating if all rows are the same       \n",
    "    if drop_cols and len(equal_cols) != 0:        \n",
    "        for pair in equal_cols:\n",
    "            \n",
    "            if pair[1] not in del_cols:\n",
    "                gdf.drop(pair[1], axis = 1, inplace = True)\n",
    "                del_cols.append(pair.pop(1))\n",
    "            else: pass\n",
    "                \n",
    "        print(f\"1. Deleted   columns: \", end = \" \") # repr without new line\n",
    "        [print(f'{col}\\t', end = \" \") for col in del_cols]; print('\\v')\n",
    "        \n",
    "        print(f\"2. Remaining columns: \", end = \" \") # repr without new line\n",
    "        [print(f'{col}\\t', end = \" \") for col in list(chain.from_iterable(equal_cols))]; print('\\v')\n",
    "        \n",
    "    # 4 // printing columns that remain after purging\n",
    "    elif len(equal_cols) == 0: print('List to return is empty')\n",
    "    else: \n",
    "        print(f\"Remaining columns: \", end = \" \") # repr without new line\n",
    "        [print(f'{col}\\t', end = \" \") for col in list(chain.from_iterable(equal_cols))] \n",
    "\n",
    "    print(f\"-- Finished task -----------------------------------------------------\\n\")\n",
    "    return list(chain.from_iterable(equal_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------ Unify ID columns if gml_id is dropped\n",
    "    \n",
    "def get_strPoint(x):\n",
    "    \"\"\"\n",
    "    Returns last part of Cadastral ID in gml_id inside shorten_localID\n",
    "    \"\"\"\n",
    "    return x.split('.')[-1]\n",
    "\n",
    "def shorten_localID(gdf, cols_to_shorten = ['gml_id']):\n",
    "    \"\"\"\n",
    "    If localId is dropped in favor of gml_id\n",
    "    Then, namespace part is purged of name\n",
    "    \"\"\"\n",
    "    print(f\"------- Checking for ID col to shorten in {gdf.name} -------------------\")\n",
    "    \n",
    "    shorted_localID = np.vectorize(get_strPoint)   \n",
    "    for col_ID in cols_to_shorten:\n",
    "        if col_ID in gdf.columns.tolist():\n",
    "            print(f'Shortening columns: {col_ID}')\n",
    "            gdf[col_ID] = shorted_localID(gdf[col_ID])\n",
    "            \n",
    "        else: print(f'Nothing to shorten')\n",
    "    \n",
    "    print(f\"-- Finished task -----------------------------------------------------\\n\")\n",
    "    \n",
    "# ------------------------------------------------------------ LAST STEP, unify columns names\n",
    "\n",
    "def rename_cols(gdf):\n",
    "    \"\"\"\n",
    "    If col not in dict, then pass.\n",
    "    This is used to unify all geojson\n",
    "    \"\"\"\n",
    "    print(f\"--------------- Renaming cols in {gdf.name} ----------------------------\")\n",
    "\n",
    "    ######################################################## dict to modify here\n",
    "    dict_cols_to_rename = {'gml_id': 'ID',\n",
    "                           'localId_part': 'ID_part',\n",
    "                           'localId_PI': 'ID_pool',\n",
    "                           'numberOfFloorsAboveGround': 'nFloors_AG',\n",
    "                           'numberOfFloorsBelowGround': 'nFloors_BG',\n",
    "                           'heightAboveGround': 'height_AG',\n",
    "                           'heightBelowGround': 'height_BG'} # should increase if necessery\n",
    "    \n",
    "    cols_to_rename = [col for col in gdf.columns.tolist() if col in dict_cols_to_rename.keys()]\n",
    "    \n",
    "    gdf.rename(columns = dict_cols_to_rename, inplace = True) # before: after\n",
    "            \n",
    "    print(f\"1. Initial name: \", end = \" \") # repr without new line\n",
    "    [print(f'{col}', end = \" \") for col in cols_to_rename]; print(\"\\v\")\n",
    "\n",
    "    print(f\"2. Final name: \", end = \" \") # repr without new line\n",
    "    [print(f'{dict_cols_to_rename[col]}', end = \" \") for col in cols_to_rename]; print(\"\\v\")\n",
    "    \n",
    "    print(f\"-- Finished task -----------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawData_infoCleaning(gdf, \n",
    "                         drop_cols = True, \n",
    "                         cols_to_separate = ['localId', 'gml_id'],\n",
    "                         datetime_col = 'beginLifespanVersion'):\n",
    "    \"\"\"\n",
    "    Pipeline towards clearer data\n",
    "    \"\"\"\n",
    "    print(f\"Initiating cleaning pipeline -----------------------------------------\\n\")\n",
    "    \n",
    "    # -- 1 -- SEARCHING FOR COLS WITHOUT DATA IN {gdf.name} -----------------------\n",
    "    droping_DupCols(gdf, drop_cols = drop_cols)\n",
    "    # -- 2 -- SEARCHING FOR UNIQUE COLS {gdf.name} --------------------------------\n",
    "    checking_forUniques(gdf)\n",
    "    # -- 3 -- SEARCHING FOR COLS TO SEPARATE {gdf.name} ---------------------------\n",
    "    separate_parts(gdf = gdf, cols = cols_to_separate)\n",
    "    # -- 4 -- SEARCHING FOR DUPLICATED INFO {gdf.name} ----------------------------\n",
    "    checking_forIdenCols(gdf, drop_cols = drop_cols)\n",
    "    # -- 5 -- REFORMATTING DATA IN {gdf.name} -------------------------------------\n",
    "    getYearOfConstruction(gdf, LifeSpanCol = datetime_col, drop_col = drop_cols)\n",
    "    shorten_localID(gdf)\n",
    "    # -- 6 -- RENAMING INFORMATION IN {gdf.name} ----------------------------------\n",
    "    rename_cols(gdf)\n",
    "    \n",
    "\n",
    "    print(f\"Closing cleaning pipeline --------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAPAS DISPONIBLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada el volumen de datos, la inspección de éstos se realiza sobre una parte de éstos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATASTRO_PATH = '../data/raw/catastro'\n",
    "\n",
    "# Dentro de los datos displibles hay 4 capas en formato geojson\n",
    "\n",
    "#building_df = gpd.read_file(f\"{CATASTRO_PATH}/A.ES.SDGC.BU.28900.building.geojson\", rows = 25000)\n",
    "buildingParts_df = gpd.read_file(f\"{CATASTRO_PATH}/A.ES.SDGC.BU.28900.buildingpart.geojson\", rows = 25000) # pt 1\n",
    "#otherConstruction_df = gpd.read_file(f\"{CATASTRO_PATH}/A.ES.SDGC.BU.28900.otherconstruction.geojson\", rows = 25000)\n",
    "cadastralParcel = gpd.read_file(f\"{CATASTRO_PATH}/A.ES.SDGC.CP.28900.cadastralparcel.geojson\", rows = 25000)\n",
    "cadastralZoning = gpd.read_file(f\"{CATASTRO_PATH}/A.ES.SDGC.CP.28900.cadastralzoning.geojson\", rows = 25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkeo de uso de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Parts Layer total memory usage: \t\t2625.128 \tKbytes\n",
      "Cadastral Parcel Layer total memory usage: \t\t2200.128 \tKbytes\n",
      "Cadastral Zoning Layer total memory usage: \t\t1269.136 \tKbytes\n"
     ]
    }
   ],
   "source": [
    "#print(f\"Building Layer total memory usage: \\t\\t\\t{building_df.memory_usage(index=True).sum()/1000} \\tKbytes\")\n",
    "print(f\"Building Parts Layer total memory usage: \\t\\t{buildingParts_df.memory_usage(index=True).sum()/1000} \\tKbytes\")\n",
    "#print(f\"Other Construction Layer total memory usage: \\t\\t{otherConstruction_df.memory_usage(index=True).sum()/1000} \\t\\tKbytes\")\n",
    "print(f\"Cadastral Parcel Layer total memory usage: \\t\\t{cadastralParcel.memory_usage(index=True).sum()/1000} \\tKbytes\")\n",
    "print(f\"Cadastral Zoning Layer total memory usage: \\t\\t{cadastralZoning.memory_usage(index=True).sum()/1000} \\tKbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NAMES OF GEO\n",
    "# IN OBJECT MAKE IT SO THAT filename == NAME\n",
    "#building_df.name = 'BU_ALL'\n",
    "buildingParts_df.name = 'BU_PARTS'\n",
    "#otherConstruction_df.name = 'BU_OTHER'\n",
    "cadastralParcel.name = 'CAD_PARCEL'\n",
    "cadastralZoning.name = 'CAD_ZONING'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Building Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En qGIS esta capa representa la parte edificada de los solares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(building_df.info())\n",
    "display(building_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "building_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GETTING RID OF USELESS COLUMNS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP NULL COLUMNS\n",
    "# There are a couple of columns that do not offer any information\n",
    "\n",
    "building_nullCols = ['numberOfFloorsAboveGround', 'endLifespanVersion']\n",
    "building_linkCols = ['documentLink', 'format', 'informationSystem']\n",
    "building_measureCols = ['horizontalGeometryEstimatedAccuracy', 'value_uom', 'sourceStatus',\n",
    "                        'horizontalGeometryEstimatedAccuracy_uom', 'horizontalGeometryReference', \n",
    "                        'officialAreaReference']\n",
    "\n",
    "building_df.drop(building_nullCols,    axis=1, inplace = True)\n",
    "building_df.drop(building_linkCols,    axis=1, inplace = True)\n",
    "building_df.drop(building_measureCols, axis=1, inplace = True)\n",
    "\n",
    "# horizontalGeometryEstimatedAccuracy is always 0.1m accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTING GROSS FLOOR AREA\n",
    "# officialAreaReference - value - value_uom are columns that refer to the same information\n",
    "\n",
    "building_df.rename(columns={\"value\": \"grossFloorArea\"}, inplace = True)\n",
    "# building_df.drop(['officialAreaReference'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLIFYING DATE COLUMNS\n",
    "# changes between them migt be because of difference between registration or CFO.\n",
    "# end column has no sense in the context of this project\n",
    "\n",
    "building_dateCols = ['beginLifespanVersion', 'beginning', 'end']\n",
    "\n",
    "# ARE beginning and end the same\n",
    "\n",
    "building_df['Equal_beg_end'] = building_df.apply(lambda x: x['beginning'] == x['end'], axis = 1)\n",
    "building_df['Equal_beg_end'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the rest of DATES columns.\n",
    "In the [bibliography](#http://www.catastro.minhap.es/webinspire/documentos/Conjuntos%20de%20datos.pdf) it says:\n",
    ">- **beginLifespanVersion**: Fecha desde cuándo se ha dado de alta en la base de datos\n",
    "catastral. \n",
    ">- **dateOfConstruction**: estructura que define la fecha de construcción. Está compuesta por dos atributos: bu-c**ore2d:beginning y bu-core2d:end**; los valores son las fechas de construcción de cada unidad constructiva, si hay más de una en el campo **“beginning” se incluye la más antigua y en el campo “end” la más moderna** Siempre se referencian al 1 de enero . \n",
    "\n",
    "**______**\n",
    "Para este proyecto, sólo nos interesa la columna beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont know what this means.. could be buildings that are demolished or abandoned\n",
    "# Lets look to the first row reference = 000207800VK56E\n",
    "# Sede Electronica del Catastro says that this building has been built in 2004, the *end* year\n",
    "# Most plausible cause is that the builing is abandoned, in construction etc, is a phase of reconstruction\n",
    "\n",
    "# conditionOfConstruction should be different\n",
    "# Lets see that\n",
    "\n",
    "c_functional, c_declined, c_ruin = 0, 0, 0\n",
    "\n",
    "for ref in building_df[building_df['Equal_beg_end'] == False]['reference'].tolist() :\n",
    "    if building_df[building_df['reference'] == ref]['conditionOfConstruction'].tolist()[0] == 'functional':\n",
    "        c_functional += 1\n",
    "    elif building_df[building_df['reference'] == ref]['conditionOfConstruction'].tolist()[0] == 'declined':\n",
    "        c_declined += 1\n",
    "    elif building_df[building_df['reference'] == ref]['conditionOfConstruction'].tolist()[0] == 'ruin':\n",
    "        c_ruin += 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(f\"FOR BUILDINGS WITH THE DIFFERENT BEGINNING AND END DATES. CONDITION OF CONSTRUCTION\")\n",
    "print(f\"Functional : \\t{c_functional}\")\n",
    "print(f\"Declined : \\t{c_declined}\")\n",
    "print(f\"Ruins : \\t{c_ruin}\")\n",
    "\n",
    "# There should be another category\n",
    "# Buildings that may have another new cadastral reference for unknown reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LETS SEE CONDITION OF CONSTRUCTION COLUMN\n",
    "building_df['conditionOfConstruction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LETS ADD another category for demolished buildings, that reflects beginning != end\n",
    "# por tanto, los edificios en donde se ha construido suelen son edificios funcionales\n",
    "# \n",
    "# Como se ha dicho más arriba, sólo nos interesa la columna BEGINNING para efectos de este proyecto\n",
    "\n",
    "building_df.drop(['Equal_beg_end', 'end', 'beginLifespanVersion'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "building_df['dateOfConstruction'] = building_df['beginning'].apply(lambda x: \n",
    "                                    dt.datetime.strptime(x,'%Y-%m-%dT%H:%M:%S'))\n",
    "\n",
    "# Out of bounds nanosecond timestamp: 1640-01-01 00:00:00\n",
    "# Pandas required YEARS to be inside de bound of 1670 - 2560,\n",
    "# Because of the nature of data, datetime methods cannot be used for this case\n",
    "\n",
    "def get_yearofConstruction(strng):\n",
    "    \"\"\"\n",
    "    Input:  string\n",
    "    Output: year as string\n",
    "    \"\"\"\n",
    "    first_w = strng.split('T')[0]\n",
    "    # Not using datetime from pandas, not valid for this case\n",
    "    return first_w.split('-')[0]\n",
    "\n",
    "building_df['yearOfConstruction'] = building_df['beginning'].apply(get_yearofConstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_df.drop(['beginning', 'dateOfConstruction'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**____________________________________________________________________________________________________________**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THERE ARE COLUMNS THAT MAY DUPLICATE INFORMATION\n",
    "# To join with the rest of data, parcels, buildingparts... share an ID\n",
    "# Which col is the ID ??\n",
    "\n",
    "cols_id = ['gml_id', 'reference', 'localId', 'namespace']\n",
    "\n",
    "# Reference == localID ?? \n",
    "# See if there are unique values, or repeteated values (this info has to be contrasted with the rest of geojson)\n",
    "\n",
    "building_df[cols_id].describe()\n",
    "\n",
    "# All elements are unique (as expected)\n",
    "# SHOULD COINCIDE WITH PARCELS\n",
    "# gml_id for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are reference and localID the same? \n",
    "building_df.apply(lambda x: x['reference'] == x['localId'], axis = 1).value_counts() # TRUE\n",
    "# What is reference Geometry ?\n",
    "building_df['referenceGeometry'].value_counts() # All true --> Dropping\n",
    "\n",
    "# Dropping reference and namespace\n",
    "building_df.drop(['reference', 'namespace', 'referenceGeometry'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**____________________________________________________________________________________________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**____________________________________________________________________________________________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Building Parts Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{buildingParts_df.info()}\\n\")\n",
    "print(f\"Shape of {buildingParts_df.name}: \\t\\t\\t     {buildingParts_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rawData_infoCleaning(buildingParts_df, \n",
    "                     drop_cols = True, \n",
    "                     cols_to_separate = ['localId', 'gml_id'], \n",
    "                     datetime_col = 'beginLifespanVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildingParts_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**________________________**\n",
    "\n",
    "**________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Other Construction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(otherConstruction_df.info())\n",
    "display(otherConstruction_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otherConstruction_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otherConstruction_df['conditionOfConstruction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otherConstruction_df[['beginLifespanVersion', 'constructionNature', 'localId']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data corresponds to open AIR POOLS\n",
    "# I don't need this dataset to begin with\n",
    "# BUT lets cleaned it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otherConstruction_Cols = ['conditionOfConstruction', 'constructionNature', 'namespace']\n",
    "otherConstruction_df.drop(otherConstruction_Cols, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otherConstruction_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using same functions as in Building Parts\n",
    "def get_PI(x):\n",
    "    return x.split('_')[1]\n",
    "\n",
    "# Separating localID\n",
    "\n",
    "otherConstruction_df['locadID_PI'] = otherConstruction_df['localId'].apply(get_PI) #not really useful\n",
    "otherConstruction_df['locadID'] = otherConstruction_df['localId'].apply(get_ID)\n",
    "\n",
    "otherConstruction_df.drop(['localId', 'locadID_PI'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING DATE\n",
    "otherConstruction_df['yearOfConstruction'] = otherConstruction_df['beginLifespanVersion'].apply(get_yearofConstruction)\n",
    "otherConstruction_df.drop(['beginLifespanVersion'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING gml_id\n",
    "otherConstruction_df['gml_id'] = otherConstruction_df['gml_id'].apply(get_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "otherConstruction_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**________________________**\n",
    "\n",
    "**________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Cadastral Parcel Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cadastralParcel.info())\n",
    "display(cadastralParcel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData_infoCleaning(cadastralParcel, \n",
    "                     drop_cols = True, \n",
    "                     cols_to_separate = ['localId', 'gml_id'], \n",
    "                     datetime_col = 'beginLifespanVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cadastralParcel.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**________________________**\n",
    "\n",
    "**________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Cadastral Zoning Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cadastralZoning.info())\n",
    "display(cadastralZoning.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData_infoCleaning(cadastralZoning, \n",
    "                     drop_cols = True, \n",
    "                     cols_to_separate = ['localId', 'gml_id'], \n",
    "                     datetime_col = 'beginLifespanVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_forEmpties(cadastralZoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cadastralZoning['estimatedAccuracy'].value_counts())\n",
    "display(cadastralZoning['estimatedAccuracy_uom'].value_counts())\n",
    "display(cadastralZoning['originalMapScaleDenominator'].value_counts())\n",
    "display(cadastralZoning['LocalisedCharacterString'].value_counts()) # son iguales ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadastralZoning_uselessCols = ['estimatedAccuracy_uom', 'originalMapScaleDenominator', 'estimatedAccuracy', \n",
    "                              'namespace', 'endLifespanVersion']\n",
    "cadastralZoning.drop(cadastralZoning_uselessCols, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is localID == nationalCadastalZoningReference\n",
    "cadastralZoning.apply(lambda x : x['localId'] == x['nationalCadastalZoningReference'], axis = 1).value_counts() # All true\n",
    "cadastralZoning.drop(['nationalCadastalZoningReference'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE YEAR (same as rest)\n",
    "cadastralZoning['yearOfConstruction'] = cadastralZoning['beginLifespanVersion'].apply(get_yearofConstruction)\n",
    "cadastralZoning.drop(['beginLifespanVersion'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cadastralZoning.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadastralZoning.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_______________**\n",
    "\n",
    "**_______________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKING MEMORY USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Building Layer total memory usage: \\t\\t\\t{building_df.memory_usage(index=True).sum()/1000} \\tKbytes\")\n",
    "print(f\"Building Parts Layer total memory usage: \\t\\t{buildingParts_df.memory_usage(index=True).sum()/1000} \\tKbytes\")\n",
    "print(f\"Other Construction Layer total memory usage: \\t\\t{otherConstruction_df.memory_usage(index=True).sum()/1000} \\tKbytes\")\n",
    "\n",
    "print(f\"Cadastral Parcel Layer total memory usage: \\t\\t{cadastralParcel.memory_usage(index=True).sum()/1000} \\tKbytes\")\n",
    "print(f\"Cadastral Zoning Layer total memory usage: \\t\\t{cadastralZoning.memory_usage(index=True).sum()/1000} \\t\\tKbytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- INITIALLY---\n",
    "\n",
    "    Building Layer total memory usage: \t\t\t4825.128 \tKbytes\n",
    "Building Parts Layer total memory usage: \t\t2625.128 \tKbytes\n",
    "Other Construction Layer total memory usage: \t774.16 \t\tKbytes\n",
    "Cadastral Parcel Layer total memory usage: \t\t2200.128 \tKbytes\n",
    "Cadastral Zoning Layer total memory usage: \t\t1269.136 \tKbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHANGE\n",
    "\n",
    "print(f\"Building Layer memory optimization: \\t\\t\\t{np.round(building_df.memory_usage(index=True).sum()/(10*4825.128), 2)} \\t%\")\n",
    "print(f\"Building Parts Layer memory optimization: \\t\\t{np.round(buildingParts_df.memory_usage(index=True).sum()/(10*2625.128), 2)} \\t%\")\n",
    "print(f\"Other Construction Layer memory optimization: \\t\\t{np.round(otherConstruction_df.memory_usage(index=True).sum()/(10*774.16), 2)} \\t%\")\n",
    "\n",
    "print(f\"Cadastral Parcel Layer memory optimization: \\t\\t{np.round(cadastralParcel.memory_usage(index=True).sum()/(10*2200.128), 2)} \\t%\")\n",
    "print(f\"Cadastral Zoning Layer memory optimization: \\t\\t{np.round(cadastralZoning.memory_usage(index=True).sum()/(10*1269.136), 2)} \\t%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_______________**\n",
    "\n",
    "**_______________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKINF WHICH COL IS BETTER TO CONNECT TABLES IN DDBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BU_id = np.array(building_df['gml_id']); display(BU_id); display(len(BU_id))\n",
    "BP_id = np.array(buildingParts_df['gml_id']); display(BP_id); display(len(BP_id))\n",
    "BO_id = np.array(otherConstruction_df['gml_id']); display(BO_id); display(len(BO_id))\n",
    "CP_id = np.array(cadastralParcel['gml_id']); display(CP_id); display(len(CP_id))\n",
    "CZ_id = np.array(cadastralZoning['gml_id']); display(CZ_id); display(len(CZ_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "geoesp_env",
   "language": "python",
   "name": "geoesp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
